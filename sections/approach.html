<div class="row">
  <div class="col-lg-12 text-center">
    <h2 class="section-heading">Approach</h2>
    <h3 class="section-subheading text-muted">How we worked toward a solution and what data we used</h3>
  </div>
</div>


<h3 id="architecture">Architecture</h3>

<p>As mentioned, the user just needs a <a href="https://www.raspberrypi.org/">Raspberry Pi</a><a href="#fn:1" id="fnref:1" title="See footnote" class="footnote">1</a>  with a <a href="https://www.raspberrypi.org/products/camera-module/">camera module</a>. We chose <a href="https://aws.amazon.com/">Amazon Web Services (AWS)</a> as our cloud infrastructure, where the video files and a database with information related to them (where they came from, when they were recorded, and information about the detected motion) are stored, and additional video processing is performed. All the information is accessible through an interactive website (like <a href="#user-interface">this one</a> you’re now visiting).</p>

<p><img src="https://raw.githubusercontent.com/smart-cam/smart-cam/gh-pages/images/Architecture2.png" alt="Architecture Diagram" title=""></p>



<h3 id="step-by-step-process">Step-by-step Process</h3>

<ol>
<li><p><strong>Motion detection</strong> is performed by the <strong>Raspberry Pi</strong>, using <a href="https://www.python.org/">Python</a> as programming language and <a href="http://opencv.org/">OpenCV</a> libraries for video processing.</p>

<ul><li>From all the motion detection algorithms available in OpenCV, we selected one<a href="#fn:2" id="fnref:2" title="See footnote" class="footnote">2</a> based on <a href="http://www.ee.surrey.ac.uk/CVSSP/Publications/papers/KaewTraKulPong-AVBS01.pdf">Gaussian Mixture Model-based foreground and background segmentation</a>: a background probability model is constructed for each pixel, adapting to background changes by incrementally updating the mean and variance of existing Gaussians, incorporating some and discarding others.</li>
<li>For each frame in the video, the background and foreground are segmented: the former will be largely static over consecutive frames<a href="#fn:3" id="fnref:3" title="See footnote" class="footnote">3</a>, so substantial changes are detected and assigned to the latter (and the background is then subtracted).</li>
<li>The foreground may contain some noise (e.g., due to small changes in lighting conditions), which would result in false positives (motion is detected in the absence of it). To reduce the presence of those, we set a threshold to the number of pixels in a (320×240) frame that correspond to the foreground, whose final value is 0.05 (3,840 pixels).</li></ul></li>
<li><p>The step above allows to <strong>upload to the cloud</strong> only those videos of interest, in which motion has been detected. The camera is constantly working, and upon motion detection a video file (of 10 seconds) is uploaded to a bucket in <a href="https://aws.amazon.com/s3/">Amazon S3</a>. Information about the video file (the Raspberry Pi ID, the timestamp, …) and the foreground size (the ratio of pixels corresponding to foreground) is also written in a <a href="https://aws.amazon.com/dynamodb/">DynamoDB</a> (NoSQL) database, in JSON format.</p></li>
<li><p>Another Python program running in an <a href="https://aws.amazon.com/ec2/">Amazon EC2</a> virtual server reads from both sources (S3 and DynamoDB) to <strong>count faces</strong> in each video. The script queries the database for still unprocessed videos, runs the face counting algorithm on those, and updates the database.</p>

<ul><li>We used a <a href="http://docs.opencv.org/3.0-alpha/modules/objdetect/doc/cascade_classification.html">cascade classifier</a> based on the <a href="https://en.wikipedia.org/wiki/Viola%E2%80%93Jones_object_detection_framework">Viola–Jones object detection framework</a>, which is capable of processing images extremely rapidly while achieving high detection rates.  <br>
First, a cascade<a href="#fn:4" id="fnref:4" title="See footnote" class="footnote">4</a> of boosted<a href="#fn:5" id="fnref:5" title="See footnote" class="footnote">5</a> classifiers is trained with a few hundred sample views of a particular object (a face, in our case), called positive examples, and negative examples (arbitrary images of similar size). After the classifer is trained, it will output a 1 if the region is likely to show the object, and 0 otherwise. To search for the object in the whole image and in different sizes, the search window is moved across and the scan procedure is done several times at different scales.</li>
<li>Doing this part of the video processing (and others that might be added in the future) in the cloud has the advantage of <strong>freeing up the Raspberry Pi from using its limited resources</strong>.</li></ul></li>
<li><p>Finally, the user can access all the videos and information stored in S3 and DynamoDB, respectively, through an interactive website (<a href="#user-interface">see below for an example</a>), where he or she can overview, zoom, filter, and drill up and down the foreground size and face count of all the videos recorded by the Raspberry Pis, as well as play them.</p></li>
</ol>



<div class="row text-center">
  <div class="col-md-4">
    <!--
                    <span class="fa-stack fa-4x">
                        <i class="fa fa-circle fa-stack-2x text-primary"></i>
                        <i class="fa fa-car fa-stack-1x fa-inverse"></i>
                    </span>
-->
    <h4 class="service-heading">STEP 1:<br />Refine the Problem</h4>
    <p class="text-muted">How do we spend our advertising budget in order to maximize ROI?</p>
    <p class="text-muted">In order to maximize ROI, we need to advertise to the individuals and regions that are most likely to purchase an electric vehicle (EV).</p>
    <p class="text-muted">How do we determine which individuals and regions are most likely to purchase an EV?</p>
  </div>
  <div class="col-md-4">
    <!--
                    <span class="fa-stack fa-4x">
                        <i class="fa fa-circle fa-stack-2x text-primary"></i>
                        <i class="fa fa-laptop fa-stack-1x fa-inverse"></i>
                    </span>
-->
    <h4 class="service-heading">STEP 2:<br />Operationalize Based on Available Data</h4>
    <p class="text-muted">The ideal dataset would contain every EV owner's defining qualities, so we could determine if any qualities are highly predictive of EV ownership.</p>
    <p class="text-muted">EV owner data is not publicly available, but EV charging station location data is available as well as demographic data.</p>
    <p class="text-muted">Use EV charging station locations as a proxy for EV ownership and use demographic data for locations with EV charging stations as a proxy for EV owner qualities.</p>
  </div>
  <div class="col-md-4">
    <!--
                    <span class="fa-stack fa-4x">
                        <i class="fa fa-circle fa-stack-2x text-primary"></i>
                        <i class="fa fa-lock fa-stack-1x fa-inverse"></i>
                    </span>
-->
    <h4 class="service-heading">STEP 3:<br />Collect and Prepare Data for Modeling</h4>
    <p class="text-muted">Conduct some background research to decide which demographic qualities are most likely to predict EV ownership. (These turned out to be income, educational attainment, and home ownership.)</p>
    <p class="text-muted">Download EV charging station and demographic data and wrangle it to merge them together, keep only the relevant columns, and deal with missing values.</p>
  </div>
</div>

<div class="row text-center">
  <div class="col-md-4">
    <!--
                    <span class="fa-stack fa-4x">
                        <i class="fa fa-circle fa-stack-2x text-primary"></i>
                        <i class="fa fa-car fa-stack-1x fa-inverse"></i>
                    </span>
-->
    <h4 class="service-heading">STEP 4:<br />Build Models for Markets</h4>
    <p class="text-muted">Use various machine learning approaches to identify the demographic attributes that are more likely to be associated with potential buyers of Electric Vehicles</p>

  </div>
  <div class="col-md-4">
    <!--
                    <span class="fa-stack fa-4x">
                        <i class="fa fa-circle fa-stack-2x text-primary"></i>
                        <i class="fa fa-laptop fa-stack-1x fa-inverse"></i>
                    </span>
-->
    <h4 class="service-heading">STEP 5:<br />Enhance Markets Information with Ad rates</h4>
    <p class="text-muted">Although the models identify the best target markets, it may not be cost effective to advertise to these locations. </p>
    <p class="text-muted">Identified and used additional market based advertisement rates to find the right balance between effectiveness and cost. </p>
  </div>

  <div class="col-md-4">
    <!--
                    <span class="fa-stack fa-4x">
                        <i class="fa fa-circle fa-stack-2x text-primary"></i>
                        <i class="fa fa-laptop fa-stack-1x fa-inverse"></i>
                    </span>
-->
    <h4 class="service-heading">STEP 6:<br />Build Marketing Tools</h4>
    <p class="text-muted">Although models and data-sets identify effective and efficient markets, the marketing team still wants to fine-tune how they use this information. So we built a dashboard that provides the information to marketing experts in easily consumable formats.</p>
  </div>


</div>
